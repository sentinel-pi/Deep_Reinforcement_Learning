{
 "cells": [
  {
   "cell_type": "code",
   "id": "f8f50fd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:11:04.607782Z",
     "start_time": "2025-10-27T18:11:02.207613Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from Buffers.ExperienceReplayBuffer import ExperienceReplay"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "8f4d23e1",
   "metadata": {},
   "source": "<center> <h1> Constants"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:11:04.681629Z",
     "start_time": "2025-10-27T18:11:04.678897Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "17d866db15ef22ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ce86656f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:11:04.699199Z",
     "start_time": "2025-10-27T18:11:04.694924Z"
    }
   },
   "source": [
    "SEED = 13 \n",
    "GAMMA = 0.995\n",
    "STEPS = 1000\n",
    "EPISODES = 300\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "CRITIC_LR = 1e-3\n",
    "ACTOR_LR = 1e-4\n",
    "POLICY_DELAY = 2\n",
    "BUFFER_SIZE = 100000\n",
    "\n",
    "#soft update parameter\n",
    "TAU = 0.005\n",
    "\n",
    "#actionNoise parameter\n",
    "NOISE_STD = 0.2\n",
    "NOISE_MEAN = 0"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "eac3b035",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:11:04.757310Z",
     "start_time": "2025-10-27T18:11:04.748119Z"
    }
   },
   "source": [
    "env = gym.make(\"Pendulum-v1\",render_mode = \"rgb_array\")\n",
    "# env = RecordVideo(env,\"../Results/PolicyBased\",lambda x: x%25 == 0 and x != 0 , fps=15 )\n",
    "actionNum = env.action_space.shape[0] \n",
    "stateNum = env.observation_space.shape[0]\n",
    "print((actionNum,stateNum))\n",
    "actionMagnitude = env.action_space.high[0]\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "e991fdd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:11:04.911368Z",
     "start_time": "2025-10-27T18:11:04.806045Z"
    }
   },
   "source": [
    "import time\n",
    "env.reset()\n",
    "for steps in range(1):\n",
    "    action = env.action_space.sample()\n",
    "    _,rewards,terminated,truncated,_=env.step(action)\n",
    "    print(rewards)\n",
    "    time.sleep(0.1)\n",
    "env.close()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.992865763509047\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "66b044d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:11:04.965139Z",
     "start_time": "2025-10-27T18:11:04.959058Z"
    }
   },
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self,stateNum,h1,h2,h3,actionNum):\n",
    "        super().__init__()\n",
    "        self.q1 = nn.Sequential(\n",
    "            nn.Linear(stateNum + actionNum,h1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h1,h2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h2,h3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h3,1),\n",
    "        )\n",
    "        self.q2 = nn.Sequential(\n",
    "            nn.Linear(stateNum + actionNum,h1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h1,h2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h2,h3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h3,1),\n",
    "        )\n",
    "\n",
    "    def forward(self,state,action):\n",
    "        x = torch.hstack((state,action))\n",
    "        q1 = self.q1(x)\n",
    "        q2 = self.q2(x)\n",
    "        return q1,q2"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "d04acc1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:11:04.979682Z",
     "start_time": "2025-10-27T18:11:04.975143Z"
    }
   },
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self,stateNum,h1,h2,h3,actionNum):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(stateNum,h1)\n",
    "        self.fc2 = nn.Linear(h1,h2)\n",
    "        self.fc3 = nn.Linear(h2,h3)\n",
    "        self.fc4 = nn.Linear(h3,actionNum)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        A = torch.tanh(self.fc4(x))* actionMagnitude\n",
    "        return A"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "ef79928c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:11:06.220608Z",
     "start_time": "2025-10-27T18:11:05.026373Z"
    }
   },
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(SEED)\n",
    "critic        = Critic(stateNum,400,200,100,actionNum).to(device)\n",
    "actor         = Actor(stateNum,400,200,100,actionNum).to(device)\n",
    "critic_target = Critic(stateNum,400,200,100,actionNum).to(device)\n",
    "actor_target  = Actor(stateNum,400,200,100,actionNum).to(device)\n",
    "critic_target.load_state_dict(critic.state_dict())\n",
    "actor_target.load_state_dict(actor.state_dict())\n",
    "\n",
    "critic_criterion = nn.MSELoss()\n",
    "critic_optim =  Adam(critic.parameters(),CRITIC_LR)\n",
    "actor_optim= Adam(actor.parameters(),ACTOR_LR)\n",
    "\n",
    "buffer = ExperienceReplay(BUFFER_SIZE,device)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "3e5ac799",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:11:06.231590Z",
     "start_time": "2025-10-27T18:11:06.228284Z"
    }
   },
   "source": [
    "def soft_update(online,target,tau):\n",
    "    online_dict = online.state_dict()\n",
    "    target_dict = target.state_dict()\n",
    "    for key in online_dict.keys():\n",
    "        target_dict[key]= tau*online_dict[key] + (1-tau) * target_dict[key]\n",
    "    target.load_state_dict(target_dict)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "5b7840ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:12:44.780530Z",
     "start_time": "2025-10-27T18:11:06.285526Z"
    }
   },
   "source": [
    "\n",
    "rewards = []\n",
    "numSteps=0\n",
    "update_steps = 0\n",
    "for episode in range(EPISODES):\n",
    "    old_observation,info =env.reset(seed=episode)\n",
    "    cumulative_reward = 0\n",
    "\n",
    "    for step in range(STEPS):\n",
    "        numSteps += 1\n",
    "        action = actor(torch.Tensor(old_observation).reshape(1,-1).to(device)).cpu().detach().numpy().squeeze()\n",
    "        exploration_noise = np.random.normal(NOISE_MEAN,NOISE_STD,env.action_space.shape[0])\n",
    "        noisyAction = np.clip(action.squeeze() + exploration_noise,-2,2)\n",
    "        old_observation = old_observation.squeeze()\n",
    "        new_observation,reward,terminated,truncated,info = env.step(noisyAction)\n",
    "        cumulative_reward+=reward\n",
    "        done = terminated or truncated\n",
    "        buffer.append(old_observation,noisyAction,reward,new_observation,done)\n",
    "        rewards.append(reward)\n",
    "        old_observation = new_observation\n",
    "        # t = time.time()\n",
    "        if(buffer.size() >= BATCH_SIZE):\n",
    "            old_state,old_action,reward,new_state,done=buffer.sample(BATCH_SIZE)\n",
    "            reward = reward.reshape(-1,1)\n",
    "            done = done.reshape(-1,1)   \n",
    "            old_action = old_action.reshape(-1,actionNum)\n",
    "\n",
    "            y_hat1,y_hat2 = critic(old_state,old_action)\n",
    "            #TODO fix this after downloading torch\n",
    "            with torch.no_grad():\n",
    "                targetAction = actor_target(new_state)\n",
    "                targetActionNoise = torch.clip(torch.normal(NOISE_MEAN,NOISE_STD,targetAction.shape,device=device),-0.2,0.2)\n",
    "                noisyTargetAction = targetAction + targetActionNoise\n",
    "                clippedNoisyAction = torch.clip(noisyTargetAction,-actionMagnitude,actionMagnitude)\n",
    "                q1,q2 = critic_target(new_state,clippedNoisyAction)\n",
    "                y = reward + GAMMA * torch.min(q1,q2) * (1.0 - done)\n",
    "\n",
    "            critic_loss1 =critic_criterion(y_hat1,y)\n",
    "            critic_loss2 =critic_criterion(y_hat2,y)\n",
    "            critic_loss=critic_loss1 + critic_loss2\n",
    "            critic_optim.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optim.step()\n",
    "            update_steps+=1\n",
    "            #delayed Update\n",
    "            if update_steps % POLICY_DELAY == 0 :\n",
    "\n",
    "                #freeze critic\n",
    "                for p in critic.parameters():\n",
    "                    p.requires_grad = False\n",
    "\n",
    "                actor_optim.zero_grad()\n",
    "                q1,q2 = critic(old_state,actor(old_state))\n",
    "                actor_loss = -q1.mean()\n",
    "                actor_loss.backward()\n",
    "                actor_optim.step()\n",
    "\n",
    "                #freeze critic\n",
    "                for p in critic.parameters():\n",
    "                    p.requires_grad = True\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    soft_update(critic, critic_target, TAU)\n",
    "                    soft_update(actor, actor_target, TAU)\n",
    "        if(truncated or terminated):\n",
    "            break;\n",
    "        # print(t-time.time())\n",
    "\n",
    "    print(f\"Episode: {episode} | Reward: {cumulative_reward},actor_loss: \")# {actor_loss.detach().item()},critic_loss: {critic_loss.detach().item()}\n",
    "    print(numSteps)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 | Reward: -1313.588210311692,actor_loss: \n",
      "200\n",
      "Episode: 1 | Reward: -1087.1692665500848,actor_loss: \n",
      "400\n",
      "Episode: 2 | Reward: -1646.1273994193343,actor_loss: \n",
      "600\n",
      "Episode: 3 | Reward: -1738.0199421379762,actor_loss: \n",
      "800\n",
      "Episode: 4 | Reward: -1806.3368187992767,actor_loss: \n",
      "1000\n",
      "Episode: 5 | Reward: -1560.8232207621038,actor_loss: \n",
      "1200\n",
      "Episode: 6 | Reward: -1384.8174479507672,actor_loss: \n",
      "1400\n",
      "Episode: 7 | Reward: -1484.519321890306,actor_loss: \n",
      "1600\n",
      "Episode: 8 | Reward: -1376.9777516744387,actor_loss: \n",
      "1800\n",
      "Episode: 9 | Reward: -1576.070241399909,actor_loss: \n",
      "2000\n",
      "Episode: 10 | Reward: -1559.6669885883591,actor_loss: \n",
      "2200\n",
      "Episode: 11 | Reward: -1544.2008874274943,actor_loss: \n",
      "2400\n",
      "Episode: 12 | Reward: -1454.711626683674,actor_loss: \n",
      "2600\n",
      "Episode: 13 | Reward: -1563.666258884555,actor_loss: \n",
      "2800\n",
      "Episode: 14 | Reward: -1523.954386298587,actor_loss: \n",
      "3000\n",
      "Episode: 15 | Reward: -1435.9676854813122,actor_loss: \n",
      "3200\n",
      "Episode: 16 | Reward: -1258.5964268640694,actor_loss: \n",
      "3400\n",
      "Episode: 17 | Reward: -1453.9439777498553,actor_loss: \n",
      "3600\n",
      "Episode: 18 | Reward: -615.2345884881189,actor_loss: \n",
      "3800\n",
      "Episode: 19 | Reward: -1129.881435764582,actor_loss: \n",
      "4000\n",
      "Episode: 20 | Reward: -1249.610632196258,actor_loss: \n",
      "4200\n",
      "Episode: 21 | Reward: -1182.8529142619852,actor_loss: \n",
      "4400\n",
      "Episode: 22 | Reward: -905.4162644740848,actor_loss: \n",
      "4600\n",
      "Episode: 23 | Reward: -903.3401404435574,actor_loss: \n",
      "4800\n",
      "Episode: 24 | Reward: -397.0379620960694,actor_loss: \n",
      "5000\n",
      "Episode: 25 | Reward: -640.9896387203162,actor_loss: \n",
      "5200\n",
      "Episode: 26 | Reward: -503.76896648608886,actor_loss: \n",
      "5400\n",
      "Episode: 27 | Reward: -523.2054190998281,actor_loss: \n",
      "5600\n",
      "Episode: 28 | Reward: -634.1346849392105,actor_loss: \n",
      "5800\n",
      "Episode: 29 | Reward: -1135.9144562459626,actor_loss: \n",
      "6000\n",
      "Episode: 30 | Reward: -369.802435845379,actor_loss: \n",
      "6200\n",
      "Episode: 31 | Reward: -446.77860550362215,actor_loss: \n",
      "6400\n",
      "Episode: 32 | Reward: -527.4779799933482,actor_loss: \n",
      "6600\n",
      "Episode: 33 | Reward: -142.56631908185182,actor_loss: \n",
      "6800\n",
      "Episode: 34 | Reward: -379.12860583468023,actor_loss: \n",
      "7000\n",
      "Episode: 35 | Reward: -264.65127737669155,actor_loss: \n",
      "7200\n",
      "Episode: 36 | Reward: -262.8371891933821,actor_loss: \n",
      "7400\n",
      "Episode: 37 | Reward: -134.16857723642352,actor_loss: \n",
      "7600\n",
      "Episode: 38 | Reward: -5.8647196977916485,actor_loss: \n",
      "7800\n",
      "Episode: 39 | Reward: -8.266900561002943,actor_loss: \n",
      "8000\n",
      "Episode: 40 | Reward: -124.27552774032097,actor_loss: \n",
      "8200\n",
      "Episode: 41 | Reward: -371.03917575009564,actor_loss: \n",
      "8400\n",
      "Episode: 42 | Reward: -123.44983381584,actor_loss: \n",
      "8600\n",
      "Episode: 43 | Reward: -131.3425436327439,actor_loss: \n",
      "8800\n",
      "Episode: 44 | Reward: -244.54060878555086,actor_loss: \n",
      "9000\n",
      "Episode: 45 | Reward: -129.5948019838877,actor_loss: \n",
      "9200\n",
      "Episode: 46 | Reward: -294.4194339412387,actor_loss: \n",
      "9400\n",
      "Episode: 47 | Reward: -122.52687137857328,actor_loss: \n",
      "9600\n",
      "Episode: 48 | Reward: -130.27686105318534,actor_loss: \n",
      "9800\n",
      "Episode: 49 | Reward: -128.5599797260246,actor_loss: \n",
      "10000\n",
      "Episode: 50 | Reward: -247.7954590178612,actor_loss: \n",
      "10200\n",
      "Episode: 51 | Reward: -328.83931385133815,actor_loss: \n",
      "10400\n",
      "Episode: 52 | Reward: -127.99458123470463,actor_loss: \n",
      "10600\n",
      "Episode: 53 | Reward: -365.7117573203643,actor_loss: \n",
      "10800\n",
      "Episode: 54 | Reward: -124.68290982370154,actor_loss: \n",
      "11000\n",
      "Episode: 55 | Reward: -244.3849498197526,actor_loss: \n",
      "11200\n",
      "Episode: 56 | Reward: -129.58295869437848,actor_loss: \n",
      "11400\n",
      "Episode: 57 | Reward: -130.226646935285,actor_loss: \n",
      "11600\n",
      "Episode: 58 | Reward: -127.51218715734643,actor_loss: \n",
      "11800\n",
      "Episode: 59 | Reward: -129.58815148395763,actor_loss: \n",
      "12000\n",
      "Episode: 60 | Reward: -124.3394119620253,actor_loss: \n",
      "12200\n",
      "Episode: 61 | Reward: -129.6685149477386,actor_loss: \n",
      "12400\n",
      "Episode: 62 | Reward: -130.539048865568,actor_loss: \n",
      "12600\n",
      "Episode: 63 | Reward: -131.15585517658184,actor_loss: \n",
      "12800\n",
      "Episode: 64 | Reward: -1474.4222334934698,actor_loss: \n",
      "13000\n",
      "Episode: 65 | Reward: -262.2992074427967,actor_loss: \n",
      "13200\n",
      "Episode: 66 | Reward: -241.34983355672003,actor_loss: \n",
      "13400\n",
      "Episode: 67 | Reward: -130.30489233593335,actor_loss: \n",
      "13600\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 10\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(STEPS):\n\u001B[1;32m      9\u001B[0m     numSteps \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m---> 10\u001B[0m     action \u001B[38;5;241m=\u001B[39m \u001B[43mactor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mold_observation\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreshape\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mnumpy()\u001B[38;5;241m.\u001B[39msqueeze()\n\u001B[1;32m     11\u001B[0m     exploration_noise \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mnormal(NOISE_MEAN,NOISE_STD,env\u001B[38;5;241m.\u001B[39maction_space\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m     12\u001B[0m     noisyAction \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mclip(action\u001B[38;5;241m.\u001B[39msqueeze() \u001B[38;5;241m+\u001B[39m exploration_noise,\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m,\u001B[38;5;241m2\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "718a90af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:12:55.058712Z",
     "start_time": "2025-10-27T18:12:47.834420Z"
    }
   },
   "source": [
    "env = gym.make(\"Pendulum-v1\",render_mode=\"human\")\n",
    "old_observation,_ =env.reset()\n",
    "for steps in range(STEPS):\n",
    "    old_observation = torch.Tensor(old_observation).to(device)\n",
    "    action = actor(old_observation).cpu().detach().numpy()\n",
    "    old_observation,_,terminated,truncated,_=env.step(action)\n",
    "    if(truncated or terminated):\n",
    "        break\n",
    "env.close()"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba49d697",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44af9e48",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
